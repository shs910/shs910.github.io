<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Huashan Sun (Â≠ôÂçéÂ±±)</title>

    <meta name="author" content="Huashan Sun (Â≠ôÂçéÂ±±)">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Huashan Sun
                </p>
                <p>
              Hi üëãüèª! I'm Huashan Sun, a second-year MS student at <a href="https://cs.bit.edu.cn/">the School of Computer Science and Technology</a>, <a href="https://www.bit.edu.cn/">Beijing Institute of Technology</a> (2023-present), under the guidance of <a href="https://cs.bit.edu.cn/szdw/jsml2/yyznyskjsyjs2/78c31a2505434740a51076b614742941.htm">Prof. Yang Gao</a>.
              I earned my bachelor's degree in Artificial Intelligence from Beijing Institute of Technology (2019-2023). Previously, I conducted internships at <a href="https://www.xiaohongshu.com/">Rednote</a> (2022.10-2023.02) and <a href="https://tongyi.aliyun.com/welcome">Tongyi, Alibaba</a> (2024.12-2025.10).
              I'm an INTJü§´, passionate about üì∏photography and ü•æhiking & exploring scenic landscapes üèîÔ∏è. <a href="https://hotpot910.tuchong.com/">Here</a> are some photos I've taken üì∑‚ú®.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sunhuashan910@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/HuashanSun-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=vsEFOtoAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.semanticscholar.org/author/Huashan-Sun/2261689091">Semantic Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/shs910">Github</a> &nbsp;/&nbsp;
                  <a href="https://hotpot910.tuchong.com/">Photography</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/HuashanSun.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/HuashanSun.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am fascinated by natural language processingü§ñüí≠ and curious about the capabilities of large language models (LLMs)  and underlying mechanisms üîç. 
                  Currently, I focus on long-context language modeling, particularly efficient training and inference üìù‚ö° (e.g., sparse attention, context compression, and long-context alignment). 
                  Previously, I worked on continual learning and long-text style modeling üîÑüé® of LLMs. 
                  My research aims to enhance models' real-world applicability while making them more controllable and safe üéØüîí.
                  Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/qwenlong1-5.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/pdf/2512.12967" id="qwenlong1-5">
          <papertitle>QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management</papertitle>
        </a>
        <br>
        Weizhou Shen, Ziyi Yang, Chenliang Li, Zhiyuan Lu, Miao Peng, <strong>Huashan Sun</strong>, Yingcheng Shi, Shengyi Liao, Shaopeng Lai, Bo Zhang, Dayiheng Liu, Fei Huang, Jingren Zhou and Ming Yan
        <br>
        <em>technical report</em>, 2025
        <br>
        <a href="https://github.com/Tongyi-Zhiwen/Qwen-Doc">[GitHub]</a>
        <a href="https://arxiv.org/pdf/2512.12967">[Paper]</a>

        <p>We introduce QwenLong-L1.5, which enhances long-context reasoning through three key innovations: a data synthesis pipeline for complex multi-hop reasoning, stabilized reinforcement learning for extended sequences, and a memory-augmented architecture for ultra-long contexts (up to 4M tokens). 
			Built on Qwen3-30B-A3B-Thinking, the model rivals top competitors like GPT-5 and Gemini-2.5-Pro, improving its baseline by 9.90 points on reasoning benchmarks and achieving a 9.48-point gain on ultra-long tasks.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/solopo.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/pdf/2505.11166" id="solopo">
          <papertitle><span class="highlight">SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization</span></papertitle>
        </a>
        <br>
        <strong>Huashan Sun<sup>*</sup></strong>, Shengyi Liao<sup>*</sup>, Yansen Han, Yu Bai, Yang Gao, Cheng Fu, Weizhou Shen, Fanqi Wan, Ming Yan, Ji Zhang and Fei Huang
        <br>
        <em>Under review</em>, 2025
        <br>
        <a href="https://github.com/shs910/SoLoPO">[GitHub]</a>
        <a href="https://arxiv.org/pdf/2505.11166v2">[Paper]</a>

        <p>We propose a framework named Short-to-Long Preference Optimization (SoLoPO), decoupling long-context preference optimization (PO) into two components: short-context PO and short-to-long reward alignment (SoLo-RA), supported by both theoretical and empirical evidence. 
          SoLoPO enhances mainstream algorithms (DPO, SimPO, ORPO) with superior performance on long-context benchmarks and improved training efficiency. 
          For example, SoLoPO reduces DPO run time by 52% and doubles the maximum trainable sequence length.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/forgetting.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/pdf/2411.11932" id="pseudo_forgetting">
          <papertitle><span class="highlight">Unveiling and Addressing Pseudo Forgetting in Large Language Models</span></papertitle>
        </a>
        <br>
        <strong>Huashan Sun</strong>, Yizhe Yang, Yinghao Li Jiawei Li, Yang Gao<br>
        <em>ACL 2025 Findings</em>, 2025
        <br>
        <a href="https://github.com/DIRECT-BIT/Reviving-Dormant-Memories">[GitHub]</a>
        <a href="https://arxiv.org/pdf/2411.11932">[Paper]</a>

        <p>We Identify and validate the pseudo forgetting phenomenon in LLMs, demonstrating that performance degradation on previously learned tasks stems from reduced instruction dependency rather than actual capability loss.
          We propose Rationale-Guidance Difficulty based Replay framework for continual learning, which effectively mitigates pseudo forgetting while maintaining model plasticity.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/psst.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/pdf/2311.08389" id="psst">
          <papertitle><span class="highlight">PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer</span></papertitle>
        </a>
        <br>
        <strong>Huashan Sun<sup>*</sup></strong>, Yixiao Wu<sup>*</sup>, Yuhao Ye, Yizhe Yang, Yinghao Li, Jiawei Li and Yang Gao<br>
        <em>EMNLP 2024 Findings</em>, 2024
        <br>
        <a href="https://github.com/DIRECT-BIT/PSST">[GitHub]</a>
        <a href="https://arxiv.org/pdf/2311.08389">[Paper]</a>

        <p>Public-Speaking Style Transfer (PSST) is an complex, passage-level, scalable, and downstream application-oriented text style transfer task. We started from linguistics and real-world data to analyze the key features of "public speaking style" and proposed fine-grained evaluation metrics for such complex long-text style transfer tasks. Comprehensive experimental results reveal that current LLMs suffer from over-stylization, uneven style strength distribution, and severe semantic degradation problems.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/mindllm.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/pdf/2310.15777" id="mindllm">
          <papertitle><span class="highlight">MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications</span></papertitle>
        </a>
        <br>
        Yizhe Yang<sup>*</sup>, <strong>Huashan Sun<sup>*</sup></strong>, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang Liu, Yang Gao and Heyan Huang<br>
        <em>AI Open</em>, 2024
        <br>
        <a href="https://huggingface.co/bit-dny/MindLLM-1b3-chat-zh-v2.0">[Huggingface]</a>
        <a href="https://arxiv.org/pdf/2310.15777">[Paper]</a>
        <p>MindLLM is a series of bilingual lightweight large language models with 1.3B and 3B parameters, trained from scratch to provide efficient alternatives to larger models. We detail the complete development process including data construction, model architecture, evaluation, and practical applications, with MindLLM achieving performance comparable to or exceeding larger open-source models on public benchmarks.</p>

      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/ica.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/pdf/2406.11474" id="ica">
          <papertitle>How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment</papertitle>
        </a>
        <br>
        Heyan Huang (Prof.), Yinghao Li, <strong>Huashan Sun</strong>, Yu Bai and Yang Gao
        <br>
        <em>EMNLP 2024</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2406.11474">[Paper]</a>
        <p>We investigate the mechanism and applicability of In-Context Alignment (ICA) through ablation (format, system prompt, and example) experiments and comprehensive evaluations.
           Our findings indicate that (1) the example part is crucial, (2) compared to fine-tuning, ICA demonstrates superior performance in knowledge-based tasks and tool-use tasks,
           (3) ICA still exhibits limitations in areas such as multi-turn dialogues and instruction following.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/survey.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://dl.acm.org/doi/10.1145/3735632" id="survey">
          <papertitle>Fundamental capabilities and applications of large language models: A survey</papertitle>
        </a>
        <br>
        Jiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao Li, <strong>Huashan Sun</strong>, Yuhang Liu, Xingpeng Si, Yuhao Ye, Yixiao Wu, Yiguan Lin, Bin Xu, Bowen Ren, Chong Feng, Yang Gao, Heyan Huang
        <br>
        <em>ACM Computing Surveys</em>, 2025
        <br>
        <a href="https://dl.acm.org/doi/10.1145/3735632">[Paper]</a>
        <p>In this survey, we review recent advances of LLMs in domain applications, aiming to summarize the fundamental capabilities and their collaboration. Furthermore, we establish connections between fundamental capabilities and specific domains, evaluating the varying importance of different capabilities. Based on our findings, we propose a reliable strategy for domains to choose more robust backbone LLMs for real-world applications.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/edubench.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/pdf/2505.16160" id="edubench">
          <papertitle>EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios</papertitle>
        </a>
        <br>
        Bin Xu<sup>*</sup>, Yu Bai<sup>*</sup>, <strong>Huashan Sun<sup>*</sup></strong>, Yiguan Lin<sup>*</sup>, Siming Liu, Xinyue Liang, Yaolin Li, Yang Gao and Heyan Huang
        <br>
        <em>Preprint</em>, 2025
        <br>
        <a href="https://github.com/ybai-nlp/EduBench">[Github]</a>
        <a href="https://arxiv.org/pdf/2505.16160">[Paper]</a>
        <p>We introduce the first comprehensive benchmark for evaluating large language models in educational settings, featuring 9 major scenarios and over 4,000 educational contexts. 
          We develop multi-dimensional evaluation metrics covering 12 key aspects important to educators and students, validated through human annotation. 
          Edubench provides a practical foundation for developing and evaluating education-focused language models.
        </p>
      </td>
    </tr>

          </tbody></table>

          
					<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Education</h2>
        <p>
          Master of Engineering Candidate, Artificial Intelligence, <strong>Beƒ≥ing Institute of Technology</strong> (2023.09 - 2026.06).
        </p>
        <p>
          Bachelor of Engineering, Artificial Intelligence, <strong>Beƒ≥ing Institute of Technology</strong> (2019.09 - 2023.06).
        </p>
      </td>
    </tr>
    </tbody></table>
    
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Experience</h2>
        <p>
          Research Intern at NLP, <strong>Tongyi, Alibaba</strong>, supervised by Dr. Ming Yan. (2024.12 - 2025.10).
        </p>
	      <p>
          Recommendation Algorithm Intern, <strong>Rednote</strong>, supervised by Dr. Man Xu. (2022.10 - 2023.02).
        </p>
      </td>
    </tr>
    </tbody></table>
          

            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website's code is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
